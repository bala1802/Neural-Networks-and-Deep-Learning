# Activation Function

Activation Function is a mathematical function applied to the output of each neuron (or node) in a Neural Network. Its purpose is to introduce non-linearity into the model, allowing the network to learn complex patterns and relationships in the data. In simplere terms, Neural Networks make sense of data that is not easily represented by straight lines or simple mathematical operations.

The purpose of this repository is to understand the various Activation Functions.

* ReLU (Rectified Linear Unit)
* Sigmoid
* Tanh (Hyperbolic Tangent)
* Leaky ReLU
* ELU (Exponential Linear Unit)
* Softmax
* GELU (Gaussian Error Linear Unit)
* SELU (Scaled Exponential Linear Unit)