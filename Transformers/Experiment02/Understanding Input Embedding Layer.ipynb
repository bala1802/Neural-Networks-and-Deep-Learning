{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer from the Transformers architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/01_InputEmbedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snippet from the `Attention Is All You Need` paper \n",
    "\n",
    "![Alt text](Embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the highlighted statements from the section `Embeddings and Softmax`\n",
    "\n",
    "- `Transformers` works with the numerical data, the input tokens are converted into the numbers. These numbers are called `Embeddings`\n",
    "- `Input Tokens` are the words from the input text\n",
    "- `Vectors of Dimension dmodel` Each token as a vector is represented with a sepcific dimension called `dmodel` (a hyperparameter)\n",
    "- `Embedding Layers` This layer is responsible for converting the tokens to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Code for Input Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(self.embedding(x) * math.sqrt(self.d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the above code with an example\n",
    "\n",
    "* Initialization:\n",
    "    - `InputEmbeddings` this class is of type `torch.nn module`\n",
    "    - `d_model` This is the dimension of each vector\n",
    "    - `vocab_size` The number of tokens present in the corpus\n",
    "    - `embedding` Embedding layer initialized with a shape - `(vocab_size, d_model)`\n",
    "* Forward block:\n",
    "    - According to the paper, the embedding weights are multiplied by `sqrt(d_model)`\n",
    "\n",
    "* Example:\n",
    "    - In our example The `d_model` is set to `4` and the `vocab_size` is set to `10`\n",
    "    - The Embedding layer is configured with the shape of `(10,4)`\n",
    "    - Now We have an input token \"cat\". This token is randomly initialized as `[0.2, 0.6, -0.1, 0.4]`\n",
    "    - According to the paper, the emebdding weights should be multiplied by `sqrt(d_model)` = `sqrt(4)` = `2`\n",
    "    - Now the input embeddings will be transformed like this,\n",
    "        * `[0.2, 0.6, -0.1, 0.4]` multiplied by `2`\n",
    "        * = `[0.2*2, 0.6*2, -0.1*2, 0.4*2]`\n",
    "        * = `[0.4, 1.2, -0.2, 0.8]`\n",
    "\n",
    "The final calculated `embeddings` are sent to the further steps in the `Transformers` mechanism\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
