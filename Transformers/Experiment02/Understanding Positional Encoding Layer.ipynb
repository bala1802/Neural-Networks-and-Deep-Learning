{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding Layer from the Transformers Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/02_PositionalEncoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snippet from the `Attention Is All You Need Paper`\n",
    "\n",
    "![Alt text](Images/02_PositionalEncoding_Paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why `Positional Encoding` is needed?\n",
    "\n",
    "In a sentence, word order matters. For example, consider these two sentences:\n",
    "\n",
    "    - \"I Love India\"\n",
    "    - \"India Loves I\"\n",
    "\n",
    "The meaning of the sentences is completely different based on the word order. Traditional models like `RNN` and `CNN` inherently capture this order, but Transformers do not. Positional encoding is used to provide this order information to Transformers.\n",
    "\n",
    "2. `Positional Encoding scheme`\n",
    "\n",
    "Positional Encoding is added to the `word embeddings` Refer [https://github.com/bala1802/Neural-Networks-and-Deep-Learning/blob/master/Transformers/Experiment02/Understanding%20Input%20Embedding%20Layer.ipynb] - (the vectors representing the words) to give the model information about the position of words. The scheme used in the paper involves adding `sinusoidal` functions of different frequencies and phases to the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the formula:\n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "1. Formula-1 calculates the value for the `even-indexed` dimension of the `positional encoding`:\n",
    "\n",
    "-   `pos` represents the position of the `token` in a sequence.\n",
    "-   `2i`  corresponds to `even indices` in the positional encoding `dimension`\n",
    "-   `d_model` represents the dimensionlity of the model (for example: the size of the `word embeddings`)\n",
    "\n",
    "Let's understand how it works:\n",
    "\n",
    "-   `2i / d_model`: This fraction scales the position value tobe within the range [0,1]. For different dimensions (`i`) this fraction varies.\n",
    "-   `10000^(2i / d_model)`: This term increases exponentially as `i` grows, creating a range of different \"frequencies\" for encoding positions.\n",
    "-   `pos / (10000^(2i / d_model))`: Diving the position value by this term scales the position value to match the range of frequencies\n",
    "-   `sin(pos / (10000^(2i / d_model)))`: Finally, the `sine` function is applied to the scaled `position value`, creating a `sinusoidal` pattern. The result is a value that varies smoothly with the position but in a periodic manner.\n",
    "\n",
    "2. Formula-2 calculates the value for the `odd-indexed` dimension of the `positional encoding`:\n",
    "\n",
    "Everything is similar to the `Formula-1`, execpt that `cosine` function is applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wavelengths in Geometric Progression & Linear Relationship for Relative Positions\n",
    "\n",
    "- `Wavelengths in Geometric Progression`: The `wavelengths` of these `sinusoids` are arranged in a geometric progression. They follow a pattern where each wavelength is a constant factor larger than the previous one. In this case, the wavelengths start from `2π` (a full cycle) and increase a factor by `10000*2π`\n",
    "\n",
    "- For any fixed offset `k` (representing the relative position of tokens), the `positional encoding` at position `PE_pos+k` can be represented as a  `linear` function of the `positional encoding` at position `PE_pos`. This `Linear` relationship helps the model capture the relative positions\n",
    "\n",
    "- Let's illustrate this with an example: Suppose we have a sequence of words: `I Love India`. And let's consider a specific dimension of the positional encoding, denoted by `Dim1`, which corresponds to a `sinusoidal` wave.\n",
    "\n",
    "    * At the `first` position `I`, the `Dim1` value of `positional encoding` might be some value based on the `sinusoid` at a wavelength of `2π`.\n",
    "\n",
    "    * At the `second` position `Love`, the `Dim1` value of `positional encoding would be different`, corresponding to the `same sinusoid` but at a `different point in its cycle`, because it's in a different position in the sequence.\n",
    "    \n",
    "    * At the `third` position `India`, the `Dim1` value of `positional encoding would be different again`, reflecting the `sinusoid` at yet another point in its cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        self.d_model: Storing the dimension of the model\n",
    "        self.seq_len: Storing the length of the sequence\n",
    "        self.dropout: Storing the dropout rate\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \"\"\"\n",
    "        Creating an empty matrix pe of shape (seq_len, d_model) to store the positional encodings.\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        \"\"\"\n",
    "        Creating a vector position of shape (seq_len, 1) with values from 0 to seq_len-1. \n",
    "        This will represent the positions of tokens in the sequence.\n",
    "        \"\"\"\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \"\"\"\n",
    "        Computing a vector div_term of shape (d_model,) with values that will be used in the positional encoding calculations.\n",
    "        \"\"\"\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \"\"\"\n",
    "        Applying Sine and Cosine functions\n",
    "            - sin(position * (10000 ** (2i / d_model)))\n",
    "            - cos(position * (10000 ** (2i / d_model)))\n",
    "        \n",
    "        To alternate indices of the `pe` matrix based on the positions and `div_term`. \n",
    "        Simulating the sinusoidal encoding pattern mentioned in the paper\n",
    "        \"\"\"\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \"\"\"\n",
    "        Adding the batch dimension to the positional encoding matrix `pe`, making it `(1, seq_len, d_model)`\n",
    "        to match the batch size of the input data.\n",
    "        \"\"\"\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        \"\"\"\n",
    "        Registering the positional encoding matrix pe as a buffer in the module. \n",
    "        A way to include non-trainable parameters that are still part of the model.\n",
    "        \"\"\"\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return(self.dropout(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the `Forward` method with an example:\n",
    "\n",
    "- `x` is an input `tensor`, which is expected to have a shape of `(batch, seq_len, d_model)`.\n",
    "\n",
    "- For example, lets say the \n",
    "\n",
    "    * `batch` size is `2`\n",
    "    * `seq_len` is `4`\n",
    "    * `d_model` (Model dimension) is `6`\n",
    "\n",
    "- So `x` = Shape of `(2,4,6)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[1, 2, 3, 4, 5, 6],\n",
    "                   [7, 8, 9, 10, 11, 12],\n",
    "                   [13, 14, 15, 16, 17, 18],\n",
    "                   [19, 20, 21, 22, 23, 24]],\n",
    "                  [[25, 26, 27, 28, 29, 30],\n",
    "                   [31, 32, 33, 34, 35, 36],\n",
    "                   [37, 38, 39, 40, 41, 42],\n",
    "                   [43, 44, 45, 46, 47, 48]]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walking through `Forward` method with `x` as an `input tensor`\n",
    "\n",
    "- `self.pe[:, :x.shape[1], :]`: Extracting the positional encoding matrix stored in `self.pe`. In this example, `x.shape[1]` is `4`, so we take the `first 4 positions` from the `positional encoding` matrix.\n",
    "\n",
    "- `x = x + (self.pe[:, :x.shape[1], :])`: `x` will be `Embeddings` of the `input data`. This statement will add the `embeddings` to the `positional` information provided by the `self.pe`. Doing this helps the model understand the order of tokens in the sequence.\n",
    "\n",
    "- The output of the `forward` method will be a `tensor` with the same shape as the `input tensor` - `(batch, seq_len, d_model)`\n",
    "\n",
    "-  `x[0] = x[0] + self.pe[:, :4, :]`  # Adding positional encoding to the first sequence in the batch\n",
    "-  `x[1] = x[1] + self.pe[:, :4, :]`  # Adding positional encoding to the second sequence in the batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
